# Neural Sign Language Translation: Gloss2Text

## Introduction
With this proposal, we aim to improvise over the baseline for the Gloss2Text task as proposed in Camgoz
et al. [2018]. We plan to implement the model inspired by our primary paperâ€“ RNN-based encoder-decoder with slight
variation i.e use BERT (bert-base-german-uncased) to form representations for the input sequence (i.e.
gloss). Further, we plan to utilize a cross-attention mechanism and conduct experiments. Training and
testing will be done on RWTH-PHOENIX-Weather 2014 T dataset and evaluation will be performed using
BLEU-4 metric. Our analysis and results comparison will be performed against the baseline.

## Dataset

## Training and Evaluation


## Results


## References
